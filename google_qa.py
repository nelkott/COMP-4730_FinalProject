# COMP-4730 Final Project
# Created by Saffa Alvi, Nour ElKott, & Nandini Patel

## Objective
# The purpose of this research data analysis project is to apply deep learning approaches to explore Natural Language Processing (NLP), and to create a model for the Google QUEST Q&A Labeling competition on Kaggle.com. 

# The objective of this competition is to use a new dataset, compiled by the CrowdSource team at Google, to create a predictive model “for different subjective aspects of question-answering” [1]. The goal of this project is to build a performative model to accurately predict the classes of the unlabeled data and to answer the research questions, defined in this report, that are related to NLP and this competition topic. Our model accuracy will also be compared to other existing models and evaluated to see which properties/characteristics of our model affect its overall accuracy. 

# The accomplishment of this research project benefits from the help and direction from our professor - Dr. Robin Gras and a few online resources, which were of great help.

## Imports
# The following libraries, functions, etc were imported to help with constructing the NLP model.

import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
import string
import os
import re

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Input
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import LSTM, Embedding, Concatenate, TimeDistributed, Bidirectional,GRU, Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D,SimpleRNN
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping

import tensorflow_hub as hub
from sklearn.metrics.pairwise import cosine_similarity

from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""## Training Set, Testing Set, and Submission File

The data provided to complete this project are the following:
<br />**train.csv**
<br />contains training set of questions and answers
<br /> **test.csv**
<br /> contains testing set of questions and answers

Here, the program reads the three .csv files and saves the contents into the corresponding files.
"""

trainingSet      = pd.read_csv('train.csv')
testingSet       = pd.read_csv('test.csv')
sampleSubmission = pd.read_csv('sample_submission.csv')

"""**Display the Target Variables in the Training Data Set**
<br/> The targets all have a value between 0 and 1, inclusive. 
"""

pd.set_option('display.max_columns', None)

targets = list(sampleSubmission.columns[1:])
trainingSet[targets].describe()

"""**Display the Training Set Size**
<br />**Training Set** is composed of:
<br />41 columns for Q&A classifications (ex. question_title, answer_helpful, etc).
<br />6079 rows for each entry.
"""

print("Training Set Size (rows, cols): ", trainingSet.shape)

trainingSetCols = trainingSet.columns
n = 1

print("\nLIST OF COLUMN NAMES IN TRAINING SET:")
print("----------------------------------------")
for i in trainingSetCols:
    print(n, ". ", i)
    n = n + 1

"""**Display the Testing Set Size**
<br />**Testing Set** is composed of:
<br />11 columns for Q&A classifications (ex. question_title, answer_user_name, etc).
<br />476 rows for each entry.
"""

print("Testing Set Size (rows, cols): ", testingSet.shape)

testingSetCols = testingSet.columns
n = 1

print("\nLIST OF COLUMN NAMES IN TESTING SET:")
print("----------------------------------------")
for i in testingSetCols:
    print(n, ". ", i)
    n = n + 1

"""**Display the contents of the Training Set**"""

trainingSet.head()

"""## Building a Sentiment Classifier
We will build a sentiment classifier and test its performance. The Sentiment Classifier will be generated by using the 'category' and 'question_body' columns of the training data. This will train the model to:
1. Mine the data: ignore useless words, characters, etc in order to focus on the important content in the data
2. Create a Recurrent Neural Network (RNN), to form an undirected graph of sequences of inputs. In this case, the inputs are the data in the **training set**.

## Begin Mining Data
We will begin to remove any contents found in each question and answer in the dataset.
The following will take place in order to keep useful information:
1. remove URLS
2. convert uppercase letters to lowercase letters
3. remove tags
4. remove words containing possible errors
5. remove special characters
6. remove 'stop words'
7. stemming and lemmatization
"""

import nltk

# will be used to remove the stop words
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize

# will be used for stemming and lemmatization
nltk.download('wordnet')
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

"""**Print a random Q&A in the training set before being processed.**"""

print("RANDOM QUESTION BODY W/OUT MINING")
print("---------------------------------------------------------------")
print(trainingSet['question_body'].values[344])

print("RANDOM ANSWER W/OUT MINING")
print("---------------------------------------------------------------")
print(trainingSet['answer'].values[344])

"""**Mining Function**
Remove any of the following texts that may be found in the questions and answers:
<br />1. Web Links
<br />2. Tags
<br />3. Upper-case letters, convert to lower-case
<br />4. Typos
<br />5. Special characters
<br />6. Lemmatize the words
"""

def mineText(text):
    text = str(re.sub(r'http\S+', '', text))
    text = str(re.sub(r'<.*?>', ' ', text))
    text = str(text.lower())
    text = str(re.sub(r"\S*\d\S*", "", text))
    text = str(re.sub('[^A-Za-z0-9]+', ' ', text))
    text = str(' '.join([word for word in text.split(' ') if word not in stopwords.words('english')]))
    text = str(' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')]))
    return text

trainingSet['question_body'] = trainingSet['question_body'].apply(mineText)
trainingSet['answer'] = trainingSet['answer'].apply(mineText)

print("RANDOM QUESTION BODY AFTER MINING")
print("---------------------------------------------------------------")
print(trainingSet['question_body'].values[344])

print("\nRANDOM ANSWER AFTER MINING")
print("---------------------------------------------------------------")
print(trainingSet['answer'].values[344])

"""## Creating a Recurrent Neural Network (RNN)

In this project, we will generate a simple model composed of the following layers:
<br /> 1. Input: 
<br /> 2. Embedding: We will do this in order to have space for more semantic nuances in sentences.
<br /> 3. Bidirectional RNN:
<br /> 4. Global Max Pooling: 
<br /> 5. Dense Layer:
<br /> 6. Dense Layer:
"""

# parameters used for typical embeddings
maxLength = 1000
maxFeatures = 5000 
embeddingSize = 768

inp = Input(shape=(maxLength,)) # returns a shape tuple of ints, size of the maxLength of 

z = Embedding(maxFeatures,embeddingSize,input_length = maxLength)(inp)
z = Bidirectional(SimpleRNN(60,return_sequences='True'))(z)
z = GlobalMaxPool1D()(z)
z = Dense(16,activation='relu')(z)
z = Dense(5,activation='softmax')(z)

model = Model(inputs=inp,outputs=z)
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

yLabel = LabelEncoder() # label encodeer
labels = yLabel.fit_transform(trainingSet['category']) # used for the labels of each category

yTrain = labels # set x training vars as the labels (categories)
# setup the training (x,y) and testing (X,Y) 'question_body' datasets.  test_size default 0.25, random_state for shuffling data
xTrain,xTest,yTrain, yTest = train_test_split(trainingSet['question_body'], yTrain, test_size=0.25,random_state=30)

tokenizer = Tokenizer(num_words = maxFeatures) # setup the tokenizer, num words in question_body to vectorize
tokenizer.fit_on_texts(list(xTrain))  

xVal = xTest # set xVal to xTest
xVal = tokenizer.texts_to_sequences(xVal) # transform xVal into sequence of integers

xTrain = tokenizer.texts_to_sequences(xTrain) # transform xTrain into sequence of integers

xTrain = pad_sequences(xTrain, maxlen = maxLength) # pad sequence so that the vectors can have the same lengths
xVal   = pad_sequences(xVal, maxlen = maxLength) # pad sequence so that the vectors can have the same lengths

yVal = yTest

print("PADDED AND TOKENIZED SEQUENCES, ALL VECTORS HAVE THE SAME LENGTH")
print("----------------------------------------------------------------")

print("xTrain Sequence, Padded and Tokenized: ", xTrain.shape)
print("yTrain                               : ", yTrain.shape)

print("xVal Sequence, Padded and Tokenized  : ", xVal.shape)
print("yVal                                 : ", yVal.shape)


model.fit(xTrain, yTrain, batch_size=128, epochs=10, verbose=2, validation_data = (xVal,yVal))

"""**Embedding Matrix**"""

model = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed")
trainingSet['question_title'] = trainingSet['question_title'].apply(mineText)

questionEmbedding = [model([trainingSet.iloc[i].question_title])[0] for i in range(trainingSet.shape[0])]

"""**Testing the Sentiment Similarity Process**"""

def sentimentSimilarity(query):
    queryEmbedding = model([query])
    similarity = cosine_similarity(questionEmbedding, queryEmbedding)
    similarityVal = [similarity[i][0] for i in range(similarity.shape[0])]
    return np.argmax(similarityVal)

question = 'I am having an issue with accessing different OpenCV libraries. What are some functions that may help solve my issue?'

print('SAMPLE QUESTION')
print("-----------------------------")
print(question)

print('\nPREPROCESSED QUESTION')
print("-----------------------------")
question = mineText(question)
print(question)

predictedQuery = sentimentSimilarity(question)

print('\nSEMANTIC SIMILARITY')
print("-----------------------------")
print(trainingSet.iloc[predictedQuery].question_title)

